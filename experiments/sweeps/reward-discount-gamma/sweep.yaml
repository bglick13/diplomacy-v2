# Ablation Study: Discounted Future Rewards (Temporal Credit Assignment)
#
# Motivation: In Diplomacy, moves often set up future captures. Spring positioning
# enables Fall SC captures, but the positioning move gets no credit for the capture.
#
# This sweep tests discount factors (gamma) that propagate future rewards backwards:
#   R_t = delta_t + gamma * R_{t+1}
#
# With gamma=0.95, a move that enables a capture 2 steps later receives 90% credit.
# This addresses the temporal credit assignment problem in multi-turn games.

metadata:
  name: "reward-discount-gamma-v2"
  description: "Ablation: Discount factor for temporal credit assignment"
  hypothesis: |
    Propagating future rewards backwards (gamma > 0) will improve learning by:
    1. Giving positioning moves credit for subsequent captures
    2. Reducing reward variance (cumulative returns are smoother than deltas)
    3. Creating more informative gradients for early-game decisions

    Expected outcomes:
    - gamma=0.95 should outperform baseline gamma=0.0 (immediate only, previous behavior)
    - Lower variance in reward signals with discounting
    - Better early-game play (Spring positioning for Fall captures)
  experiment_tag_prefix: "ablation-gamma"
  created: "2024-12-25"
  author: "claude-code"

# Default config values shared across all runs
defaults:
  total_steps: 100
  # Keep other reward settings at defaults to isolate gamma effect
  step_reward_weight: 0.8
  final_reward_weight: 0.2
  # Start fresh (don't resume from failed runs)
  disable_auto_resume: true

  # STABILITY TUNING: KL penalty prevents runaway policy divergence
  # Without KL penalty, policy can diverge too fast (KL explodes 0.03 â†’ 0.5 in 6 steps)
  # Small penalty + warmup allows initial exploration then constrains drift
  kl_beta: 0.01            # Small KL penalty to constrain divergence
  kl_beta_warmup_steps: 10 # Allow 10 steps of free exploration before penalty kicks in

  learning_rate: 5e-6      # Reduced from 1e-5 - previous runs exploded at step 10
  max_grad_norm: 5.0       # Conservative safety net (was 10.0, too permissive)
  lora_rank: 32            # Keep higher capacity
  entropy_coef: 0.01       # Prevents mode collapse to empty responses

  # PPO tuning: Wide bounds to accommodate vLLM-HuggingFace logprobs mismatch (~0.8-1.5x)
  # IS correction handles extreme outliers; PPO clipping handles real policy drift
  ppo_epsilon_low: 0.2     # ratio >= 0.8 (allows HF computing lower logprobs)
  ppo_epsilon_high: 0.5    # ratio <= 1.5 (allows HF computing higher logprobs)

  # Disable token-level loss weighting to test if it's causing instability
  # With variable-length completions, longer samples dominate gradients
  use_token_level_loss: false

# Analysis configuration
analysis:
  primary_metric: "trueskill/display_rating"
  secondary_metrics:
    - "benchmark/reward_mean"
    - "benchmark/reward_std"  # Expect lower variance with discounting
    - "game/avg_sc_count"
    - "game/win_bonus_rate"
  expected_ranking: ["B", "C", "A"]  # gamma=0.95 expected to win, baseline last
  success_criteria:
    - "At least one gamma > 0 outperforms baseline (A, gamma=0.0)"
    - "Reward variance decreases with discounting"
    - "No training instability"

# Run definitions (3 variants: baseline + moderate discount + full future credit)
runs:
  A:
    name: "baseline-immediate-only"
    description: "gamma=0.0 - No future credit, only immediate step delta (previous behavior)"
    config:
      reward_discount_gamma: 0.0
      experiment_tag: "${metadata.experiment_tag_prefix}-A-gamma000"

  B:
    name: "moderate-discount"
    description: "gamma=0.95 - Moderate discounting, ~60% weight on step+10"
    config:
      reward_discount_gamma: 0.95
      experiment_tag: "${metadata.experiment_tag_prefix}-B-gamma095"

  C:
    name: "full-future-credit"
    description: "gamma=1.0 - Undiscounted sum of all future step rewards"
    config:
      reward_discount_gamma: 1.0
      experiment_tag: "${metadata.experiment_tag_prefix}-C-gamma100"
