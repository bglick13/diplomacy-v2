# Ablation Study: Discounted Future Rewards (Temporal Credit Assignment)
#
# Motivation: In Diplomacy, moves often set up future captures. Spring positioning
# enables Fall SC captures, but the positioning move gets no credit for the capture.
#
# This sweep tests discount factors (gamma) that propagate future rewards backwards:
#   R_t = delta_t + gamma * R_{t+1}
#
# With gamma=0.95, a move that enables a capture 2 steps later receives 90% credit.
# This addresses the temporal credit assignment problem in multi-turn games.

metadata:
  name: "reward-discount-gamma-v2"
  description: "Ablation: Discount factor for temporal credit assignment"
  hypothesis: |
    Propagating future rewards backwards (gamma > 0) will improve learning by:
    1. Giving positioning moves credit for subsequent captures
    2. Reducing reward variance (cumulative returns are smoother than deltas)
    3. Creating more informative gradients for early-game decisions

    Expected outcomes:
    - gamma=0.95 should outperform baseline gamma=0.0 (immediate only, previous behavior)
    - Lower variance in reward signals with discounting
    - Better early-game play (Spring positioning for Fall captures)
  experiment_tag_prefix: "ablation-gamma"
  created: "2024-12-25"
  author: "claude-code"

# Default config values shared across all runs
defaults:
  total_steps: 100
  # Keep other reward settings at defaults to isolate gamma effect
  step_reward_weight: 0.8
  final_reward_weight: 0.2
  # Start fresh (don't resume from failed runs)
  disable_auto_resume: true

  # ==========================================================================
  # STABILITY CONFIG (from stability-ablation and mlp-ablation experiments)
  # ==========================================================================
  # Key findings:
  # - MLP modules enable 3x more learning (+250 Elo gap vs +84 attention-only)
  # - MLP + KL penalty crashed at step 57 with kl_mean=0.267
  # - Attention-only was stable but plateaued
  # Solution: MLP + stronger KL penalty (0.02 vs 0.01) for stability

  # KL penalty: Slightly higher to prevent MLP instability
  kl_beta: 0.02            # Increased from 0.01 - MLP needs more constraint
  kl_beta_warmup_steps: 5  # Shorter warmup - get KL penalty active sooner

  learning_rate: 5e-6      # Conservative LR for stability
  max_grad_norm: 5.0       # Conservative gradient clipping

  # LoRA: MLP modules for learning capacity (attention-only plateaus)
  lora_rank: 16            # Reduced from 32 - rank 32 alone didn't help
  lora_target_modules:     # MLP modules enable learning new knowledge
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

  # Other stability settings
  entropy_coef: 0.0        # Disabled - didn't help in ablation
  use_ppo_clipping: false  # Disabled - didn't help in ablation
  use_token_level_loss: false  # Disabled - can cause gradient issues

# Analysis configuration
analysis:
  # Use fixed reference metrics (win rate against dynamic league is misleading)
  primary_metric: "elo/base_model"  # Lower = better (league beats base model)
  secondary_metrics:
    - "elo_gap"  # Best checkpoint - base_model (higher = better)
    - "elo/defensive_bot"  # Lower = exploiting fixed strategies
    - "kl/mean"  # Stability indicator
    - "kl/max"   # Stability indicator
  expected_ranking: ["B", "C", "A"]  # gamma=0.95 expected to win, baseline last
  success_criteria:
    - "At least one gamma > 0 achieves lower base_model Elo than gamma=0.0"
    - "No crashes (all runs complete 100 steps)"
    - "KL mean stays < 0.3 throughout training"
  analysis_command: |
    uv run python .claude/skills/experiment-analysis/analyze_sweep.py --sweep ablation-gamma

# Run definitions (3 variants: baseline + moderate discount + full future credit)
runs:
  A:
    name: "baseline-immediate-only"
    description: "gamma=0.0 - No future credit, only immediate step delta (previous behavior)"
    config:
      reward_discount_gamma: 0.0
      experiment_tag: "${metadata.experiment_tag_prefix}-A-gamma000"

  B:
    name: "moderate-discount"
    description: "gamma=0.95 - Moderate discounting, ~60% weight on step+10"
    config:
      reward_discount_gamma: 0.95
      experiment_tag: "${metadata.experiment_tag_prefix}-B-gamma095"

  C:
    name: "full-future-credit"
    description: "gamma=1.0 - Undiscounted sum of all future step rewards"
    config:
      reward_discount_gamma: 1.0
      experiment_tag: "${metadata.experiment_tag_prefix}-C-gamma100"
