# Ablation Study: Discounted Future Rewards (Temporal Credit Assignment)
#
# Motivation: In Diplomacy, moves often set up future captures. Spring positioning
# enables Fall SC captures, but the positioning move gets no credit for the capture.
#
# This sweep tests discount factors (gamma) that propagate future rewards backwards:
#   R_t = delta_t + gamma * R_{t+1}
#
# With gamma=0.95, a move that enables a capture 2 steps later receives 90% credit.
# This addresses the temporal credit assignment problem in multi-turn games.

metadata:
  name: "reward-discount-gamma-v2"
  description: "Ablation: Discount factor for temporal credit assignment"
  hypothesis: |
    Propagating future rewards backwards (gamma > 0) will improve learning by:
    1. Giving positioning moves credit for subsequent captures
    2. Reducing reward variance (cumulative returns are smoother than deltas)
    3. Creating more informative gradients for early-game decisions

    Expected outcomes:
    - gamma=0.95 should outperform baseline gamma=0.0 (immediate only, previous behavior)
    - Lower variance in reward signals with discounting
    - Better early-game play (Spring positioning for Fall captures)
  experiment_tag_prefix: "ablation-gamma"
  created: "2024-12-25"
  author: "claude-code"

# Default config values shared across all runs
defaults:
  total_steps: 100
  # Keep other reward settings at defaults to isolate gamma effect
  step_reward_weight: 0.8
  final_reward_weight: 0.2
  # Note: kl_beta=0.0 (default) is intentional for domain-specific RL.
  # We WANT to diverge from base model. Stability comes from PPO clipping,
  # not KL regularization. (Bug fix: PPO clipping now works properly)
  # Start fresh (don't resume from failed runs)
  disable_auto_resume: true

  # STABILITY TUNING: Balance aggressive learning with stability
  # Original collapse was due to: 2x LR + 2x grad_norm + no working PPO + no entropy bonus
  # Now that PPO clipping works and entropy bonus is added, we can be more aggressive.

  learning_rate: 1e-5      # Keep aggressive - PPO clipping protects us
  max_grad_norm: 5.0       # Conservative safety net (was 10.0, too permissive)
  lora_rank: 32            # Keep higher capacity
  entropy_coef: 0.01       # Prevents mode collapse to empty responses

  # PPO tuning: Current clip_fraction ~30% is high (target: 10-20%)
  # Slightly tighten epsilon to reduce wasted compute on clipped updates
  ppo_epsilon_low: 0.15    # Was 0.2, tighter = less aggressive decreases
  ppo_epsilon_high: 0.20   # Was 0.28, tighter = less aggressive increases

# Analysis configuration
analysis:
  primary_metric: "trueskill/display_rating"
  secondary_metrics:
    - "benchmark/reward_mean"
    - "benchmark/reward_std"  # Expect lower variance with discounting
    - "game/avg_sc_count"
    - "game/win_bonus_rate"
  expected_ranking: ["B", "C", "A"]  # gamma=0.95 expected to win, baseline last
  success_criteria:
    - "At least one gamma > 0 outperforms baseline (A, gamma=0.0)"
    - "Reward variance decreases with discounting"
    - "No training instability"

# Run definitions (3 variants: baseline + moderate discount + full future credit)
runs:
  A:
    name: "baseline-immediate-only"
    description: "gamma=0.0 - No future credit, only immediate step delta (previous behavior)"
    config:
      reward_discount_gamma: 0.0
      experiment_tag: "${metadata.experiment_tag_prefix}-A-gamma000"

  B:
    name: "moderate-discount"
    description: "gamma=0.95 - Moderate discounting, ~60% weight on step+10"
    config:
      reward_discount_gamma: 0.95
      experiment_tag: "${metadata.experiment_tag_prefix}-B-gamma095"

  C:
    name: "full-future-credit"
    description: "gamma=1.0 - Undiscounted sum of all future step rewards"
    config:
      reward_discount_gamma: 1.0
      experiment_tag: "${metadata.experiment_tag_prefix}-C-gamma100"
